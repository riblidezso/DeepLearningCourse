{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nb_01_baby_steps.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riblidezso/DeepLearningCourse/blob/master/nb_01_baby_steps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SlY6-tlM0Pcf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Baby steps in deep learning\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tORtAyzl-2hK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load useful python packages"
      ]
    },
    {
      "metadata": {
        "id": "lSTgGCOC0XX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plotting and numerical basics\n",
        "%pylab inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eaRg3s1N0evC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# machine learning baselines\n",
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wBGBOvdV1UWO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keras deep learning framework (with tensorflow backend)\n",
        "import keras\n",
        "from keras.models import Model\n",
        "import keras.layers as kl\n",
        "import keras.regularizers as kr\n",
        "import keras.optimizers as ko"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g3xs6GXc5UX9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MNIST handwritten digits dataset"
      ]
    },
    {
      "metadata": {
        "id": "IOprHOQQ084n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WxYY1xqa4pXi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Check some handwritten digits"
      ]
    },
    {
      "metadata": {
        "id": "QmFz_b464bvI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = 4\n",
        "imshow(x_train[i])\n",
        "print('Label:', y_train[i] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SV94_cdD5IG6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Some more info about the dataset"
      ]
    },
    {
      "metadata": {
        "id": "XWsLOlrq5Mg3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wvkn-WsC_fTZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normalize pixel values to 0-1 range"
      ]
    },
    {
      "metadata": {
        "id": "iUtzCbS76B9Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_YFFQAb_i4-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define functions for 'simple' machine learning baselines"
      ]
    },
    {
      "metadata": {
        "id": "Cxunmr078_MG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_baseline(model, x_train, y_train, N_train=60000, **kwargs):\n",
        "  \"\"\"Train a baseline sklearn model.\"\"\"\n",
        "  x_train_flat = x_train.reshape(x_train.shape[0],-1)  # flatten\n",
        "  clf = model(**kwargs)  # init machine learning model\n",
        "  clf.fit(x_train_flat[:N_train],y_train[:N_train])  # train it\n",
        "  return clf\n",
        "\n",
        "\n",
        "def test_baseline(clf, x_test, y_test):\n",
        "  \"\"\"Evaluate a baseline sklearn model.\"\"\"\n",
        "  x_test_flat = x_test.reshape(x_test.shape[0],-1)  # flatten\n",
        "  y_pred = clf.predict(x_test_flat)  # make predictions\n",
        "  acc = np.equal(y_pred, y_test).mean()  # calculate accuracy\n",
        "  print(clf.__class__.__name__, 'accuracy',acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pKJ-f-BA_pUT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test logistic regression\n",
        "\n",
        "[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)"
      ]
    },
    {
      "metadata": {
        "id": "22FhFqH_5dVX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "clf = train_baseline(LogisticRegression, x_train, y_train)\n",
        "test_baseline(clf, x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UsVCO4fa_-S-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test Random Forest\n",
        "\n",
        "[Random Forest](https://en.wikipedia.org/wiki/Random_forest)"
      ]
    },
    {
      "metadata": {
        "id": "GR4-9Pc48VWu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "clf = train_baseline(RandomForestClassifier, x_train, y_train,\n",
        "                    n_jobs=-1, n_estimators = 300)\n",
        "test_baseline(clf, x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cFnv2QJ_Cs7J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test a simple neural network"
      ]
    },
    {
      "metadata": {
        "id": "vj5aM27zP-S2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "clf = train_baseline(MLPClassifier, x_train, y_train)\n",
        "test_baseline(clf, x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WdQhqG88QzYN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# see the params\n",
        "clf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AZlPR705Qk1w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Implement our own neural network for more control!"
      ]
    },
    {
      "metadata": {
        "id": "9eq1NvvxPZId",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N1 = 100\n",
        "inp = kl.Input(shape=(28*28,),name='input')  # the input data tensor\n",
        "x = kl.Dense(N1, activation='relu')(inp)  # first dense layer\n",
        "x = kl.Dense(10, activation='softmax')(x)  # prediction layer\n",
        "clf = Model(inputs=inp, outputs=x)  # define the model\n",
        "clf.compile(loss='sparse_categorical_crossentropy',optimizer='adam',\n",
        "           metrics=['accuracy'])\n",
        "clf.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cj6_7DP1R7EA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train it\n",
        "clf.fit(x_train.reshape(x_train.shape[0],-1), y_train, epochs=10,batch_size=256,\n",
        "        validation_data=(x_test.reshape(x_test.shape[0],-1), y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aWVMy5zsUyJN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Try a larger network? "
      ]
    },
    {
      "metadata": {
        "id": "BQuMdVS8SnOm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N1,N2 = 1024,512\n",
        "inp = kl.Input(shape=(28*28,),name='input')\n",
        "x = kl.Dense(N1, activation='relu')(inp)  # first dense layer\n",
        "x = kl.Dropout(0.25)(x)\n",
        "x = kl.Dense(N2, activation='relu')(x)  # first dense layer\n",
        "x = kl.Dropout(0.25)(x)\n",
        "x = kl.Dense(10, activation='softmax')(x)  #prediction layers\n",
        "clf = Model(inputs=inp, outputs=x)\n",
        "clf.compile(loss='sparse_categorical_crossentropy',optimizer='adam',\n",
        "           metrics=['accuracy'])\n",
        "clf.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nCdVUJwrTPVk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train it\n",
        "clf.fit(x_train.reshape(x_train.shape[0],-1),y_train, epochs=10, batch_size=256,\n",
        "        validation_data=(x_test.reshape(x_test.shape[0],-1), y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0YwDpXiNVa6Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional neural networks!"
      ]
    },
    {
      "metadata": {
        "id": "yUFwkajOOsFx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# need to create 1 ' color channel'\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gIDD66cFVjXJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N1,N2,N3 = 32,64,128\n",
        "inp = kl.Input(shape=(28,28,1),name='input')\n",
        "x = kl.Conv2D(N1, kernel_size=(3, 3), activation='relu')(inp)\n",
        "x = kl.Conv2D(N1, (3, 3), activation='relu')(x)\n",
        "x = kl.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = kl.Conv2D(N2, (3, 3), activation='relu')(x)\n",
        "x = kl.Conv2D(N2, (3, 3), activation='relu')(x)\n",
        "x = kl.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = kl.Dropout(0.25)(x)\n",
        "x = kl.Flatten()(x)\n",
        "x = kl.Dense(N3, activation='relu')(x)  # first dense layer\n",
        "x = kl.Dropout(0.5)(x)\n",
        "x = kl.Dense(10, activation='softmax')(x)  #prediction layers\n",
        "clf = Model(inputs=inp, outputs=x)\n",
        "clf.compile(loss='sparse_categorical_crossentropy',optimizer='adam',\n",
        "           metrics=['accuracy'])\n",
        "clf.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_ShxAqQWYc9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train it\n",
        "clf.fit(x_train,y_train, epochs=10, validation_split=0.1, batch_size=256,\n",
        "        validation_data = (x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bxQ_PsbXYgpN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "# 97.5 or 99.4 meh? Let's see a more impressive example: CIFAR10\n",
        "\n",
        "2.5% vs 0.6% not so meh btw\n",
        "\n",
        "[CIFAR10 website](https://www.cs.toronto.edu/~kriz/cifar.html)"
      ]
    },
    {
      "metadata": {
        "id": "Jy5oaMhmY8W8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "mnist_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog',\n",
        "                 'horse','ship', 'truck']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8sKRv3Eyc_C5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test yourself at home! \n",
        "\n",
        "I can do around 96%"
      ]
    },
    {
      "metadata": {
        "id": "AMr3HKuQZDPq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = 7\n",
        "imshow(x_train[i])\n",
        "plt.axis('off')\n",
        "print('Label:', mnist_classes[ int(y_train[i]) ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KvoY-YSUdVE7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Some info"
      ]
    },
    {
      "metadata": {
        "id": "zlKtkvg6dYaI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "strh_HjpdPyG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Normalize data"
      ]
    },
    {
      "metadata": {
        "id": "7iZrVCLPcPLZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hWJ17ayRdf3v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Baselines\n",
        "\n",
        "Logistic regression takes forever!"
      ]
    },
    {
      "metadata": {
        "id": "2RK6D9lldhjd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "clf = train_baseline(RandomForestClassifier, x_train, y_train.flatten(),\n",
        "                    n_jobs=-1, n_estimators = 100)\n",
        "test_baseline(clf, x_test, y_test.flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFDinMXnePj4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# OK, so this is much harder. Let's see neural nets!"
      ]
    },
    {
      "metadata": {
        "id": "JiemAVx2fRrg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### First a simple MLP "
      ]
    },
    {
      "metadata": {
        "id": "e0yviJBxeO7q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N1 = 200\n",
        "inp = kl.Input(shape=(32*32*3,),name='input')  # the input data tensor\n",
        "x = kl.Dense(N1, activation='relu')(inp)  # first dense layer\n",
        "x = kl.Dense(10, activation='softmax')(x)  # prediction layer\n",
        "clf = Model(inputs=inp, outputs=x)  # define the model\n",
        "clf.compile(loss='sparse_categorical_crossentropy',optimizer='adam',\n",
        "           metrics=['accuracy'])\n",
        "print(clf.summary())\n",
        "\n",
        "# Train it\n",
        "clf.fit(x_train.reshape(x_train.shape[0],-1), y_train, epochs=10,batch_size=256,\n",
        "        validation_data=(x_test.reshape(x_test.shape[0],-1), y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TiSBhhQyigQP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Maybe a larger one?"
      ]
    },
    {
      "metadata": {
        "id": "MDzodQKC1Kin",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N1,N2 = 1024,512\n",
        "inp = kl.Input(shape=(32*32*3,),name='input')\n",
        "x = kl.Dense(N1, activation='relu')(inp)  # first dense layer\n",
        "x = kl.Dropout(0.25)(x)\n",
        "x = kl.Dense(N2, activation='relu')(x)  # first dense layer\n",
        "x = kl.Dropout(0.25)(x)\n",
        "x = kl.Dense(10, activation='softmax')(x)  #prediction layers\n",
        "clf = Model(inputs=inp, outputs=x)\n",
        "clf.compile(loss='sparse_categorical_crossentropy',optimizer='adam',\n",
        "           metrics=['accuracy'])\n",
        "print(clf.summary())\n",
        "\n",
        "# Train it\n",
        "clf.fit(x_train.reshape(x_train.shape[0],-1), y_train, epochs=20,batch_size=256,\n",
        "        validation_data=(x_test.reshape(x_test.shape[0],-1), y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PN0ugLCIfq5V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convolutional, the same as before?"
      ]
    },
    {
      "metadata": {
        "id": "B7OCGeddftNq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N1,N2,N3 = 32,64,128\n",
        "inp = kl.Input(shape=(32,32,3),name='input')\n",
        "x = kl.Conv2D(N1, kernel_size=(3, 3), activation='relu')(inp)\n",
        "x = kl.Conv2D(N1, (3, 3), activation='relu')(x)\n",
        "x = kl.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = kl.Conv2D(N2, (3, 3), activation='relu')(x)\n",
        "x = kl.Conv2D(N2, (3, 3), activation='relu')(x)\n",
        "x = kl.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = kl.Dropout(0.25)(x)\n",
        "x = kl.Flatten()(x) \n",
        "x = kl.Dense(N3, activation='relu')(x)  # first dense layer\n",
        "x = kl.Dropout(0.5)(x)\n",
        "x = kl.Dense(10, activation='softmax')(x)  #prediction layers\n",
        "clf = Model(inputs=inp, outputs=x)\n",
        "clf.compile(loss='sparse_categorical_crossentropy',optimizer='adam',\n",
        "           metrics=['accuracy'])\n",
        "clf.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i6oN6IUEfzAm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train it\n",
        "clf.fit(x_train,y_train, epochs=30, validation_split=0.1, batch_size=256,\n",
        "        validation_data = (x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fMbBdxA_gscg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## State of the art is around 97.88% [ArXiv link](https://arxiv.org/abs/1709.01507)\n"
      ]
    },
    {
      "metadata": {
        "id": "G9bF3o6VnPB6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Finally let's implementone of the state of the art networks: [ResNext](https://arxiv.org/abs/1611.05431)\n",
        "\n",
        "![resnext](https://cdn-images-1.medium.com/max/1600/1*mdiQTfovOXKnqzfj727b9Q.png)\n",
        "\n",
        "\n",
        "Does not achieve the described results, tell me if you could fix it :)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WiBwri9U0Jc6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Train resnext of cifar10.\n",
        "The implementation follows the FAIR github repo:\n",
        "https://github.com/facebookresearch/ResNeXt\n",
        "which is slightly different than the arxiv report.\n",
        "Here I use the non-preactivation blocks.\n",
        "The specfic settings (lr,batch size ) can be changed \n",
        "to follow the original values (with enough GPUs, and time).\n",
        "Author: Dezso Ribli\n",
        "\"\"\"\n",
        "\n",
        "CARDINALITY = 2  # 16\n",
        "LR = 0.05  # 0.025\n",
        "BATCH_SIZE = 64  # 32\n",
        "EPOCHS_DROP = [150,225]\n",
        "N_EPOCHS = 250\n",
        "AUG = True\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Activation\n",
        "from keras.layers import BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras import optimizers\n",
        "import math\n",
        "\n",
        "\n",
        "def resnext(inp, resxt_block, cardinality=4):\n",
        "    \"\"\"Return resnext.\"\"\"\n",
        "    # inital conv\n",
        "    x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(5e-4))(inp)\n",
        "    x = Activation('relu')(BatchNormalization()(x))\n",
        "    # residual blocks\n",
        "    x = resxt_blocks(x, resxt_block, cardinality, 64, 256, 1)\n",
        "    x = resxt_blocks(x, resxt_block, cardinality, 128, 512, 2)\n",
        "    x = resxt_blocks(x, resxt_block, cardinality, 256, 1024, 2)\n",
        "    # classifier\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(10,activation='softmax')(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resxt_blocks(x, resxt_block, cardinality, n_ch1, n_ch2, init_stride, \n",
        "                 n_block=3):\n",
        "    \"\"\"Perform same size residual blocks.\"\"\"\n",
        "    x_shortcut = Conv2D(n_ch2, (1, 1), strides = init_stride, \n",
        "                        padding='same', kernel_regularizer=l2(5e-4))(x)\n",
        "    x_shortcut = BatchNormalization()(x_shortcut)\n",
        "    # first block\n",
        "    x = resxt_block(x, x_shortcut, cardinality, n_ch1, n_ch2, init_stride)\n",
        "    for i in range(n_block-1):  # the other residual blocks\n",
        "        x = resxt_block(x, x, cardinality, n_ch1, n_ch2)\n",
        "    return x  \n",
        "\n",
        "\n",
        "def resxt_block_b(x, x_shortcut, cardinality, n_ch1, n_ch2, init_stride=1):\n",
        "    \"\"\"Perform a residual block.\"\"\"\n",
        "    groups=[]\n",
        "    for i in range(cardinality):\n",
        "        y = Conv2D(n_ch1, (1, 1), strides=init_stride, \n",
        "                   kernel_regularizer=l2(5e-4), padding='same')(x)\n",
        "        y = Activation('relu')(BatchNormalization()(y))\n",
        "        y = Conv2D(n_ch1, (3, 3), padding='same', \n",
        "                   kernel_regularizer=l2(5e-4),)(y)\n",
        "        y = Activation('relu')(BatchNormalization()(y))\n",
        "        groups.append(y)\n",
        "    x = keras.layers.concatenate(groups)\n",
        "    x = Conv2D(n_ch2, (1, 1), padding='same', \n",
        "               kernel_regularizer=l2(5e-4),)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = keras.layers.add([x, x_shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x   \n",
        "\n",
        "\n",
        "def norm(x):\n",
        "    \"\"\"Normalize images.\"\"\"\n",
        "    x = x.astype('float32')\n",
        "    x[...,0] = (x[...,0] - x[...,0].mean())/x[...,0].std()\n",
        "    x[...,1] = (x[...,1] - x[...,1].mean())/x[...,1].std()\n",
        "    x[...,2] = (x[...,2] - x[...,2].mean())/x[...,2].std()\n",
        "    return x\n",
        "\n",
        "\n",
        "def step_decay(epoch, base_lr=LR, drop=0.1, epochs_drops=EPOCHS_DROP):\n",
        "    \"\"\"Helper for step learning rate decay.\"\"\"\n",
        "    lrate = base_lr\n",
        "    for epoch_drop in epochs_drops:\n",
        "        lrate *= math.pow(drop,math.floor(epoch/epoch_drop))\n",
        "        return lrate\n",
        "\n",
        "\n",
        "\n",
        "# SGD\n",
        "sgd = optimizers.SGD(lr=LR, decay=0, momentum=0.9, nesterov=True)\n",
        "\n",
        "# resnext\n",
        "res = resnext(Input(shape=(32,32,3)), resxt_block_b, CARDINALITY)\n",
        "res.compile(loss='sparse_categorical_crossentropy',\n",
        "            optimizer=sgd, metrics=['accuracy'])\n",
        "print(res.summary())  # print summary\n",
        "\n",
        "# load data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = map(norm, (x_train, x_test))  # normalize\n",
        "\n",
        "# train on generator with standard data augmentation\n",
        "gen = ImageDataGenerator(width_shift_range=0.125,\n",
        "                         height_shift_range=0.125,\n",
        "                         horizontal_flip=True)\n",
        "train_generator = gen.flow(x_train, y_train,\n",
        "                           batch_size=BATCH_SIZE)\n",
        "# train\n",
        "res.fit_generator(train_generator, epochs=N_EPOCHS,\n",
        "                  steps_per_epoch = len(x_train)/BATCH_SIZE,\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  callbacks=[LearningRateScheduler(step_decay)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ksW526crWCu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}